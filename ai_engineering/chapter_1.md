Notes of AI enginerring. 

# Why do you use tokens instead of Unicode? 
- Capared to characters, tokens allow models to hold meaningful components. 
- There are fewer unique tokens than unique words, reducing the modes vocabulary size. 
- Tokens also help the model process unknown words, for example chatgpting can decode the text to chatpt and ing”

There are two types of Language Models. 
- Autoregressive LM - are trained to predict missing tokens at the end of a sequence.  
- Masked language models -  can predict missing tokens anywhere in the sequence, using the context of before and after. For example, My favorite ___ team? 


# What is self-supervision in models?

# Why do larger models need more data? 
- Larger models have more capacity to learn, and, therefore, would need more training data to maximize their performance.8 You can train a large model on a small dataset too, but it’d be a waste of compute.


## What is data modalities?  

Foundation models refers to models that can handle a lot of performance. 
Large multimodal models are image detection models. 


AI engineering is engineering ontop of a fondational model 